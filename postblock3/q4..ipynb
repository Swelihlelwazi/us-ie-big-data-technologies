{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swelihlelwazi/us-ie-big-data-technologies/blob/master/postblock3/q4..ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QbPsmEf6ljt"
      },
      "source": [
        "# Purpose\n",
        "\n",
        "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
        "\n",
        "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-RHL4bg4u0_"
      },
      "source": [
        "# Setup\n",
        "\n",
        "## PostgreSQL\n",
        "\n",
        "Firstly, let's install postgres in the this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhmGVh22JcNo",
        "outputId": "f84c612c-e4b8-462d-b046-e6740cf6e278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "  logrotate netbase postgresql-14 postgresql-client-14\n",
            "  postgresql-client-common postgresql-common ssl-cert sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "  logrotate netbase postgresql postgresql-14 postgresql-client-14\n",
            "  postgresql-client-common postgresql-common postgresql-contrib ssl-cert\n",
            "  sysstat\n",
            "0 upgraded, 14 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 18.4 MB of archives.\n",
            "After this operation, 51.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.13-0ubuntu0.22.04.1 [1,225 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.13-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 18.4 MB in 2s (10.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 14.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.030-1build3_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.030-1build3) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../06-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../07-postgresql-client-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../09-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../10-postgresql-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../11-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../12-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../13-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.030-1build3) ...\n",
            "Setting up postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install postgresql postgresql-contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajhL0Z_-KK8r",
        "outputId": "151eeaa6-5f28-4a02-9b0b-f7e07f55421d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ],
      "source": [
        "!service postgresql start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P48P8Vt6Fm9"
      },
      "source": [
        "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25UVuzVNdKs",
        "outputId": "504625fa-dfda-4cf9-c10e-63ae26a80fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n"
          ]
        }
      ],
      "source": [
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW1kucySWAKv"
      },
      "source": [
        "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
        "\n",
        "We'll use the notebook magic `%end`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as0Zs9kL6PY0",
        "outputId": "c2ed7e50-4a55-49aa-96e2-0bff3ba5da85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PGPASSWORD=test\n"
          ]
        }
      ],
      "source": [
        "%env PGPASSWORD=test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGqYbg366efu"
      },
      "source": [
        "## Pagila\n",
        "\n",
        "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qICjoP_dKS8G",
        "outputId": "2cbbc59b-fe39-47bb-f49c-271b04997d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pagila'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 94 (delta 47), reused 85 (delta 42), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (94/94), 2.91 MiB | 15.58 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/spatialedge-ai/pagila.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYHVKYqSMthy",
        "outputId": "736324f6-eab1-4bab-88a3-208e06963118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATE DATABASE\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -c \"create database pagila\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfgNogz3MSq_",
        "outputId": "daa85ea0-5446-4262-b33f-62b040b908bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "CREATE TYPE\n",
            "ALTER TYPE\n",
            "CREATE DOMAIN\n",
            "ALTER DOMAIN\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "SET\n",
            "SET\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE AGGREGATE\n",
            "ALTER AGGREGATE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zpqaYNZPABo",
        "outputId": "652a0ac8-2c7c-483b-842a-5a736bc58a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "COPY 200\n",
            "COPY 109\n",
            "COPY 600\n",
            "COPY 603\n",
            "COPY 16\n",
            "COPY 2\n",
            "COPY 599\n",
            "COPY 6\n",
            "COPY 1000\n",
            "COPY 5462\n",
            "COPY 1000\n",
            "COPY 4581\n",
            "COPY 2\n",
            "COPY 16044\n",
            "COPY 1157\n",
            "COPY 2312\n",
            "COPY 5644\n",
            "COPY 6754\n",
            "COPY 182\n",
            "COPY 0\n",
            " setval \n",
            "--------\n",
            "    200\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    605\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "     16\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    600\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    109\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    599\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   1000\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   4581\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      6\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  32098\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  16049\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0a4GiI6yyr"
      },
      "source": [
        "## PySpark Setup\n",
        "\n",
        "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCiCzTg-Jx2Q",
        "outputId": "dfd619aa-8ee5-4ed7-d731-b7c4e1dcb5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 09:52:20--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1046274 (1022K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.5.0.jar’\n",
            "\n",
            "postgresql-42.5.0.j 100%[===================>]   1022K  5.69MB/s    in 0.2s    \n",
            "\n",
            "2024-11-03 09:52:20 (5.69 MB/s) - ‘postgresql-42.5.0.jar’ saved [1046274/1046274]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BQsxrwZBhWc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "%config Completer.use_jedi = False\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
        "\n",
        "# print(os.environ['SPARK_HOME'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1owkTgHVBuix",
        "outputId": "aeeb2de0-7ff1-411e-beb3-0a8543692c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "--2024-11-03 09:52:44--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 300971569 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.1-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.2.1-bin-had 100%[===================>] 287.03M  29.8MB/s    in 11s     \n",
            "\n",
            "2024-11-03 09:52:56 (26.4 MB/s) - ‘spark-3.2.1-bin-hadoop3.2.tgz’ saved [300971569/300971569]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
        "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ighjc_WdUNgC"
      },
      "outputs": [],
      "source": [
        "!cp postgresql-42.5.0.jar spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8DvgX7OEHWo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCIQhdSYC5uh",
        "outputId": "4640bae7-fe9b-486f-fc11-4645f7116bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reMhwdxpCz05",
        "outputId": "d32c937b-a5ed-46b2-e990-146249135ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "postgresql-42.2.5.jar\n",
            "env: PYARROW_IGNORE_TIMEZONE=1\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config(\"spark.jars\",\n",
        "                                                       \"postgresql-42.2.5.jar\").config(\n",
        "                                                          \"spark.driver.extraClassPath\",\n",
        "                                                          f\"spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\"\n",
        "                                                       ).getOrCreate()\n",
        "print(spark.conf.get('spark.jars'))\n",
        "\n",
        "%env PYARROW_IGNORE_TIMEZONE=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqG_Hk4YXuC7"
      },
      "source": [
        "# Questions\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnrQk09jQyaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e38f647-b8df-4d38-884d-e301d60b0aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "env: PGPASSWORD=test\n",
            "fatal: destination path 'pagila' already exists and is not an empty directory.\n",
            "ERROR:  database \"pagila\" already exists\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "psql:pagila/pagila-schema.sql:29: ERROR:  type \"mpaa_rating\" already exists\n",
            "ALTER TYPE\n",
            "psql:pagila/pagila-schema.sql:39: ERROR:  type \"year\" already exists\n",
            "ALTER DOMAIN\n",
            "psql:pagila/pagila-schema.sql:56: ERROR:  function \"_group_concat\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:73: ERROR:  function \"film_in_stock\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:90: ERROR:  function \"film_not_in_stock\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:135: ERROR:  function \"get_customer_balance\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:157: ERROR:  function \"inventory_held_by_customer\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:194: ERROR:  function \"inventory_in_stock\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:212: ERROR:  function \"last_day\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:227: ERROR:  function \"last_updated\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:241: ERROR:  relation \"customer_customer_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "SET\n",
            "SET\n",
            "psql:pagila/pagila-schema.sql:265: ERROR:  relation \"customer\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:329: ERROR:  function \"rewards_report\" already exists with same argument types\n",
            "ALTER FUNCTION\n",
            "psql:pagila/pagila-schema.sql:341: ERROR:  function \"group_concat\" already exists with same argument types\n",
            "ALTER AGGREGATE\n",
            "psql:pagila/pagila-schema.sql:355: ERROR:  relation \"actor_actor_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:369: ERROR:  relation \"actor\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:383: ERROR:  relation \"category_category_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:396: ERROR:  relation \"category\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:410: ERROR:  relation \"film_film_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:434: ERROR:  relation \"film\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:447: ERROR:  relation \"film_actor\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:460: ERROR:  relation \"film_category\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:483: ERROR:  relation \"actor_info\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:497: ERROR:  relation \"address_address_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:515: ERROR:  relation \"address\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:529: ERROR:  relation \"city_city_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:543: ERROR:  relation \"city\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:557: ERROR:  relation \"country_country_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:570: ERROR:  relation \"country\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:595: ERROR:  relation \"customer_list\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:618: ERROR:  relation \"film_list\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:632: ERROR:  relation \"inventory_inventory_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:646: ERROR:  relation \"inventory\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:660: ERROR:  relation \"language_language_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:673: ERROR:  relation \"language\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:696: ERROR:  relation \"nicer_but_slower_film_list\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:710: ERROR:  relation \"payment_payment_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:727: ERROR:  relation \"payment\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:743: ERROR:  relation \"payment_p2020_01\" already exists\n",
            "psql:pagila/pagila-schema.sql:744: ERROR:  \"payment_p2020_01\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:760: ERROR:  relation \"payment_p2020_02\" already exists\n",
            "psql:pagila/pagila-schema.sql:761: ERROR:  \"payment_p2020_02\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:777: ERROR:  relation \"payment_p2020_03\" already exists\n",
            "psql:pagila/pagila-schema.sql:778: ERROR:  \"payment_p2020_03\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:794: ERROR:  relation \"payment_p2020_04\" already exists\n",
            "psql:pagila/pagila-schema.sql:795: ERROR:  \"payment_p2020_04\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:811: ERROR:  relation \"payment_p2020_05\" already exists\n",
            "psql:pagila/pagila-schema.sql:812: ERROR:  \"payment_p2020_05\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:828: ERROR:  relation \"payment_p2020_06\" already exists\n",
            "psql:pagila/pagila-schema.sql:829: ERROR:  \"payment_p2020_06\" is already a partition\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:843: ERROR:  relation \"rental_rental_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:860: ERROR:  relation \"rental\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:879: ERROR:  relation \"sales_by_film_category\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:893: ERROR:  relation \"staff_staff_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:914: ERROR:  relation \"staff\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:928: ERROR:  relation \"store_store_id_seq\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:942: ERROR:  relation \"store\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:964: ERROR:  relation \"sales_by_store\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:985: ERROR:  relation \"staff_list\" already exists\n",
            "ALTER TABLE\n",
            "psql:pagila/pagila-schema.sql:995: ERROR:  multiple primary keys for table \"actor\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1003: ERROR:  multiple primary keys for table \"address\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1011: ERROR:  multiple primary keys for table \"category\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1019: ERROR:  multiple primary keys for table \"city\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1027: ERROR:  multiple primary keys for table \"country\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1035: ERROR:  multiple primary keys for table \"customer\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1043: ERROR:  multiple primary keys for table \"film_actor\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1051: ERROR:  multiple primary keys for table \"film_category\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1059: ERROR:  multiple primary keys for table \"film\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1067: ERROR:  multiple primary keys for table \"inventory\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1075: ERROR:  multiple primary keys for table \"language\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1083: ERROR:  multiple primary keys for table \"rental\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1091: ERROR:  multiple primary keys for table \"staff\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1099: ERROR:  multiple primary keys for table \"store\" are not allowed\n",
            "psql:pagila/pagila-schema.sql:1106: ERROR:  relation \"film_fulltext_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1113: ERROR:  relation \"idx_actor_last_name\" already exists\n",
            "psql:pagila/pagila-schema.sql:1120: ERROR:  relation \"idx_fk_address_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1127: ERROR:  relation \"idx_fk_city_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1134: ERROR:  relation \"idx_fk_country_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1141: ERROR:  relation \"idx_fk_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1148: ERROR:  relation \"idx_fk_film_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1155: ERROR:  relation \"idx_fk_inventory_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1162: ERROR:  relation \"idx_fk_language_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1169: ERROR:  relation \"idx_fk_original_language_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1176: ERROR:  relation \"idx_fk_payment_p2020_01_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1183: ERROR:  relation \"idx_fk_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1190: ERROR:  relation \"idx_fk_payment_p2020_01_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1197: ERROR:  relation \"idx_fk_payment_p2020_02_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1204: ERROR:  relation \"idx_fk_payment_p2020_02_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1211: ERROR:  relation \"idx_fk_payment_p2020_03_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1218: ERROR:  relation \"idx_fk_payment_p2020_03_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1225: ERROR:  relation \"idx_fk_payment_p2020_04_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1232: ERROR:  relation \"idx_fk_payment_p2020_04_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1239: ERROR:  relation \"idx_fk_payment_p2020_05_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1246: ERROR:  relation \"idx_fk_payment_p2020_05_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1253: ERROR:  relation \"idx_fk_payment_p2020_06_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1260: ERROR:  relation \"idx_fk_payment_p2020_06_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1267: ERROR:  relation \"idx_fk_store_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1274: ERROR:  relation \"idx_last_name\" already exists\n",
            "psql:pagila/pagila-schema.sql:1281: ERROR:  relation \"idx_store_id_film_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1288: ERROR:  relation \"idx_title\" already exists\n",
            "psql:pagila/pagila-schema.sql:1295: ERROR:  relation \"idx_unq_manager_staff_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1302: ERROR:  relation \"idx_unq_rental_rental_date_inventory_id_customer_id\" already exists\n",
            "psql:pagila/pagila-schema.sql:1309: ERROR:  relation \"payment_p2020_01_customer_id_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1316: ERROR:  relation \"payment_p2020_02_customer_id_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1323: ERROR:  relation \"payment_p2020_03_customer_id_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1330: ERROR:  relation \"payment_p2020_04_customer_id_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1337: ERROR:  relation \"payment_p2020_05_customer_id_idx\" already exists\n",
            "psql:pagila/pagila-schema.sql:1344: ERROR:  relation \"payment_p2020_06_customer_id_idx\" already exists\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "psql:pagila/pagila-schema.sql:1435: ERROR:  trigger \"film_fulltext_trigger\" for relation \"film\" already exists\n",
            "psql:pagila/pagila-schema.sql:1442: ERROR:  trigger \"last_updated\" for relation \"actor\" already exists\n",
            "psql:pagila/pagila-schema.sql:1449: ERROR:  trigger \"last_updated\" for relation \"address\" already exists\n",
            "psql:pagila/pagila-schema.sql:1456: ERROR:  trigger \"last_updated\" for relation \"category\" already exists\n",
            "psql:pagila/pagila-schema.sql:1463: ERROR:  trigger \"last_updated\" for relation \"city\" already exists\n",
            "psql:pagila/pagila-schema.sql:1470: ERROR:  trigger \"last_updated\" for relation \"country\" already exists\n",
            "psql:pagila/pagila-schema.sql:1477: ERROR:  trigger \"last_updated\" for relation \"customer\" already exists\n",
            "psql:pagila/pagila-schema.sql:1484: ERROR:  trigger \"last_updated\" for relation \"film\" already exists\n",
            "psql:pagila/pagila-schema.sql:1491: ERROR:  trigger \"last_updated\" for relation \"film_actor\" already exists\n",
            "psql:pagila/pagila-schema.sql:1498: ERROR:  trigger \"last_updated\" for relation \"film_category\" already exists\n",
            "psql:pagila/pagila-schema.sql:1505: ERROR:  trigger \"last_updated\" for relation \"inventory\" already exists\n",
            "psql:pagila/pagila-schema.sql:1512: ERROR:  trigger \"last_updated\" for relation \"language\" already exists\n",
            "psql:pagila/pagila-schema.sql:1519: ERROR:  trigger \"last_updated\" for relation \"rental\" already exists\n",
            "psql:pagila/pagila-schema.sql:1526: ERROR:  trigger \"last_updated\" for relation \"staff\" already exists\n",
            "psql:pagila/pagila-schema.sql:1533: ERROR:  trigger \"last_updated\" for relation \"store\" already exists\n",
            "psql:pagila/pagila-schema.sql:1541: ERROR:  constraint \"address_city_id_fkey\" for relation \"address\" already exists\n",
            "psql:pagila/pagila-schema.sql:1549: ERROR:  constraint \"city_country_id_fkey\" for relation \"city\" already exists\n",
            "psql:pagila/pagila-schema.sql:1557: ERROR:  constraint \"customer_address_id_fkey\" for relation \"customer\" already exists\n",
            "psql:pagila/pagila-schema.sql:1565: ERROR:  constraint \"customer_store_id_fkey\" for relation \"customer\" already exists\n",
            "psql:pagila/pagila-schema.sql:1573: ERROR:  constraint \"film_actor_actor_id_fkey\" for relation \"film_actor\" already exists\n",
            "psql:pagila/pagila-schema.sql:1581: ERROR:  constraint \"film_actor_film_id_fkey\" for relation \"film_actor\" already exists\n",
            "psql:pagila/pagila-schema.sql:1589: ERROR:  constraint \"film_category_category_id_fkey\" for relation \"film_category\" already exists\n",
            "psql:pagila/pagila-schema.sql:1597: ERROR:  constraint \"film_category_film_id_fkey\" for relation \"film_category\" already exists\n",
            "psql:pagila/pagila-schema.sql:1605: ERROR:  constraint \"film_language_id_fkey\" for relation \"film\" already exists\n",
            "psql:pagila/pagila-schema.sql:1613: ERROR:  constraint \"film_original_language_id_fkey\" for relation \"film\" already exists\n",
            "psql:pagila/pagila-schema.sql:1621: ERROR:  constraint \"inventory_film_id_fkey\" for relation \"inventory\" already exists\n",
            "psql:pagila/pagila-schema.sql:1629: ERROR:  constraint \"inventory_store_id_fkey\" for relation \"inventory\" already exists\n",
            "psql:pagila/pagila-schema.sql:1637: ERROR:  constraint \"payment_p2020_01_customer_id_fkey\" for relation \"payment_p2020_01\" already exists\n",
            "psql:pagila/pagila-schema.sql:1645: ERROR:  constraint \"payment_p2020_01_rental_id_fkey\" for relation \"payment_p2020_01\" already exists\n",
            "psql:pagila/pagila-schema.sql:1653: ERROR:  constraint \"payment_p2020_01_staff_id_fkey\" for relation \"payment_p2020_01\" already exists\n",
            "psql:pagila/pagila-schema.sql:1661: ERROR:  constraint \"payment_p2020_02_customer_id_fkey\" for relation \"payment_p2020_02\" already exists\n",
            "psql:pagila/pagila-schema.sql:1669: ERROR:  constraint \"payment_p2020_02_rental_id_fkey\" for relation \"payment_p2020_02\" already exists\n",
            "psql:pagila/pagila-schema.sql:1677: ERROR:  constraint \"payment_p2020_02_staff_id_fkey\" for relation \"payment_p2020_02\" already exists\n",
            "psql:pagila/pagila-schema.sql:1685: ERROR:  constraint \"payment_p2020_03_customer_id_fkey\" for relation \"payment_p2020_03\" already exists\n",
            "psql:pagila/pagila-schema.sql:1693: ERROR:  constraint \"payment_p2020_03_rental_id_fkey\" for relation \"payment_p2020_03\" already exists\n",
            "psql:pagila/pagila-schema.sql:1701: ERROR:  constraint \"payment_p2020_03_staff_id_fkey\" for relation \"payment_p2020_03\" already exists\n",
            "psql:pagila/pagila-schema.sql:1709: ERROR:  constraint \"payment_p2020_04_customer_id_fkey\" for relation \"payment_p2020_04\" already exists\n",
            "psql:pagila/pagila-schema.sql:1717: ERROR:  constraint \"payment_p2020_04_rental_id_fkey\" for relation \"payment_p2020_04\" already exists\n",
            "psql:pagila/pagila-schema.sql:1725: ERROR:  constraint \"payment_p2020_04_staff_id_fkey\" for relation \"payment_p2020_04\" already exists\n",
            "psql:pagila/pagila-schema.sql:1733: ERROR:  constraint \"payment_p2020_05_customer_id_fkey\" for relation \"payment_p2020_05\" already exists\n",
            "psql:pagila/pagila-schema.sql:1741: ERROR:  constraint \"payment_p2020_05_rental_id_fkey\" for relation \"payment_p2020_05\" already exists\n",
            "psql:pagila/pagila-schema.sql:1749: ERROR:  constraint \"payment_p2020_05_staff_id_fkey\" for relation \"payment_p2020_05\" already exists\n",
            "psql:pagila/pagila-schema.sql:1757: ERROR:  constraint \"payment_p2020_06_customer_id_fkey\" for relation \"payment_p2020_06\" already exists\n",
            "psql:pagila/pagila-schema.sql:1765: ERROR:  constraint \"payment_p2020_06_rental_id_fkey\" for relation \"payment_p2020_06\" already exists\n",
            "psql:pagila/pagila-schema.sql:1773: ERROR:  constraint \"payment_p2020_06_staff_id_fkey\" for relation \"payment_p2020_06\" already exists\n",
            "psql:pagila/pagila-schema.sql:1781: ERROR:  constraint \"rental_customer_id_fkey\" for relation \"rental\" already exists\n",
            "psql:pagila/pagila-schema.sql:1789: ERROR:  constraint \"rental_inventory_id_fkey\" for relation \"rental\" already exists\n",
            "psql:pagila/pagila-schema.sql:1797: ERROR:  constraint \"rental_staff_id_fkey\" for relation \"rental\" already exists\n",
            "psql:pagila/pagila-schema.sql:1805: ERROR:  constraint \"staff_address_id_fkey\" for relation \"staff\" already exists\n",
            "psql:pagila/pagila-schema.sql:1813: ERROR:  constraint \"staff_store_id_fkey\" for relation \"staff\" already exists\n",
            "psql:pagila/pagila-schema.sql:1821: ERROR:  constraint \"store_address_id_fkey\" for relation \"store\" already exists\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "psql:pagila/pagila-data.sql:224: ERROR:  duplicate key value violates unique constraint \"actor_pkey\"\n",
            "DETAIL:  Key (actor_id)=(1) already exists.\n",
            "CONTEXT:  COPY actor, line 1\n",
            "psql:pagila/pagila-data.sql:341: ERROR:  duplicate key value violates unique constraint \"country_pkey\"\n",
            "DETAIL:  Key (country_id)=(1) already exists.\n",
            "CONTEXT:  COPY country, line 1\n",
            "psql:pagila/pagila-data.sql:949: ERROR:  duplicate key value violates unique constraint \"city_pkey\"\n",
            "DETAIL:  Key (city_id)=(1) already exists.\n",
            "CONTEXT:  COPY city, line 1\n",
            "psql:pagila/pagila-data.sql:1560: ERROR:  duplicate key value violates unique constraint \"address_pkey\"\n",
            "DETAIL:  Key (address_id)=(1) already exists.\n",
            "CONTEXT:  COPY address, line 1\n",
            "psql:pagila/pagila-data.sql:1584: ERROR:  duplicate key value violates unique constraint \"category_pkey\"\n",
            "DETAIL:  Key (category_id)=(1) already exists.\n",
            "CONTEXT:  COPY category, line 1\n",
            "psql:pagila/pagila-data.sql:1594: ERROR:  duplicate key value violates unique constraint \"store_pkey\"\n",
            "DETAIL:  Key (store_id)=(1) already exists.\n",
            "CONTEXT:  COPY store, line 1\n",
            "psql:pagila/pagila-data.sql:2201: ERROR:  duplicate key value violates unique constraint \"customer_pkey\"\n",
            "DETAIL:  Key (customer_id)=(1) already exists.\n",
            "CONTEXT:  COPY customer, line 1\n",
            "psql:pagila/pagila-data.sql:2215: ERROR:  duplicate key value violates unique constraint \"language_pkey\"\n",
            "DETAIL:  Key (language_id)=(1) already exists.\n",
            "CONTEXT:  COPY language, line 1\n",
            "psql:pagila/pagila-data.sql:3223: ERROR:  duplicate key value violates unique constraint \"film_pkey\"\n",
            "DETAIL:  Key (film_id)=(1) already exists.\n",
            "CONTEXT:  COPY film, line 1: \"1\tACADEMY DINOSAUR\tA Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The C...\"\n",
            "psql:pagila/pagila-data.sql:8693: ERROR:  duplicate key value violates unique constraint \"film_actor_pkey\"\n",
            "DETAIL:  Key (actor_id, film_id)=(1, 1) already exists.\n",
            "CONTEXT:  COPY film_actor, line 1\n",
            "psql:pagila/pagila-data.sql:9701: ERROR:  duplicate key value violates unique constraint \"film_category_pkey\"\n",
            "DETAIL:  Key (film_id, category_id)=(1, 6) already exists.\n",
            "CONTEXT:  COPY film_category, line 1\n",
            "psql:pagila/pagila-data.sql:14290: ERROR:  duplicate key value violates unique constraint \"inventory_pkey\"\n",
            "DETAIL:  Key (inventory_id)=(1) already exists.\n",
            "CONTEXT:  COPY inventory, line 1\n",
            "psql:pagila/pagila-data.sql:14300: ERROR:  duplicate key value violates unique constraint \"staff_pkey\"\n",
            "DETAIL:  Key (staff_id)=(1) already exists.\n",
            "CONTEXT:  COPY staff, line 1\n",
            "psql:pagila/pagila-data.sql:30352: ERROR:  duplicate key value violates unique constraint \"rental_pkey\"\n",
            "DETAIL:  Key (rental_id)=(2) already exists.\n",
            "CONTEXT:  COPY rental, line 1\n",
            "COPY 1157\n",
            "COPY 2312\n",
            "COPY 5644\n",
            "COPY 6754\n",
            "COPY 182\n",
            "COPY 0\n",
            " setval \n",
            "--------\n",
            "    200\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    605\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "     16\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    600\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    109\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    599\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   1000\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   4581\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      6\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  32098\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  16049\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n",
            "--2024-11-03 10:01:51--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1046274 (1022K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.5.0.jar.1’\n",
            "\n",
            "postgresql-42.5.0.j 100%[===================>]   1022K  5.91MB/s    in 0.2s    \n",
            "\n",
            "2024-11-03 10:01:52 (5.91 MB/s) - ‘postgresql-42.5.0.jar.1’ saved [1046274/1046274]\n",
            "\n",
            "--2024-11-03 10:01:56--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 300971569 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.1-bin-hadoop3.2.tgz.1’\n",
            "\n",
            "spark-3.2.1-bin-had 100%[===================>] 287.03M  27.2MB/s    in 12s     \n",
            "\n",
            "2024-11-03 10:02:08 (24.8 MB/s) - ‘spark-3.2.1-bin-hadoop3.2.tgz.1’ saved [300971569/300971569]\n",
            "\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- store_id: integer (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- address_id: integer (nullable = true)\n",
            " |-- activebool: boolean (nullable = true)\n",
            " |-- create_date: date (nullable = true)\n",
            " |-- last_update: timestamp (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Install PostgreSQL\n",
        "!sudo apt install postgresql postgresql-contrib\n",
        "!service postgresql start\n",
        "\n",
        "# 2. Set up PostgreSQL credentials\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\"\n",
        "%env PGPASSWORD=test\n",
        "\n",
        "# 3. Clone and setup Pagila database\n",
        "!git clone https://github.com/spatialedge-ai/pagila.git\n",
        "!psql -h localhost -U postgres -c \"create database pagila\"\n",
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\"\n",
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\"\n",
        "\n",
        "# 4. Set up PySpark with JDBC\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!cp postgresql-42.5.0.jar spark-3.2.1-bin-hadoop3.2/jars\n",
        "!pip install findspark\n",
        "\n",
        "# 5. Initialize Spark\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "# 6. Create Spark Session and read customer table\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # 7. Read customer table and print schema\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Print the schema\n",
        "customer_df.printSchema()\n",
        "\n",
        "# pyspark code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXhnjaylCFI1"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTGwAFhYpanl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6db0c8-55b6-4783-a60b-3b4feacbcaaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique email addresses: 599\n"
          ]
        }
      ],
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Count unique email addresses\n",
        "unique_email_count = customer_df.select(\"email\").distinct().count()\n",
        "\n",
        "print(f\"Number of unique email addresses: {unique_email_count}\")# pyspark code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7To_5dCRGb"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Repeat this calculation using only the Dataframe API and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTO78anmCa37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc6106a-b5a8-42b3-c535-008f9b59c464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique email addresses using DataFrame API: 599\n"
          ]
        }
      ],
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Using DataFrame API to count unique emails\n",
        "unique_email_count = customer_df.dropDuplicates([\"email\"]).count()\n",
        "\n",
        "print(f\"Number of unique email addresses using DataFrame API: {unique_email_count}\")# pyspark code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IIL4RDSCcn4"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Get unique emails DataFrame using DataFrame API\n",
        "unique_emails_df = customer_df.dropDuplicates([\"email\"])\n",
        "\n",
        "                                        # Get number of partitions\n",
        "num_partitions = unique_emails_df.rdd.getNumPartitions()\n",
        "\n",
        "print(f\"Number of partitions in the unique emails DataFrame: {num_partitions}\")\n",
        "\n",
        "                                        # Alternatively, we can see the partitions in action\n",
        "print(\"\\nPartition distribution:\")\n",
        "print(unique_emails_df.rdd.glom().map(len).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPkwCeQ0tzXU",
        "outputId": "b0abe5e8-402a-4b6d-df87-5494c63c7484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in the unique emails DataFrame: 1\n",
            "\n",
            "Partition distribution:\n",
            "[599]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_6o4oLIC5SJ"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import min, max\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Calculate min and max dates using DataFrame API\n",
        "date_stats = customer_df.select(\n",
        "                                            min(\"create_date\").alias(\"earliest_date\"),\n",
        "                                                max(\"create_date\").alias(\"latest_date\")\n",
        "                                                ).collect()[0]\n",
        "\n",
        "print(f\"Earliest customer create date: {date_stats['earliest_date']}\")\n",
        "print(f\"Latest customer create date: {date_stats['latest_date']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV75BUiGuA89",
        "outputId": "082ae387-0311-4385-84b2-ed103c59aff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earliest customer create date: 2020-02-14\n",
            "Latest customer create date: 2020-02-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vndZmoyC-Ay"
      },
      "source": [
        "### Question 6.1\n",
        "\n",
        "Determine which first names occur more than once:\n",
        "\n",
        "1. using the Spark SQL API (printing the result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Register the DataFrame as a temporary view\n",
        "customer_df.createOrReplaceTempView(\"customer\")\n",
        "\n",
        "                                        # SQL query to find duplicate first names\n",
        "sql_result = spark.sql(\"\"\"\n",
        "                                            SELECT first_name, COUNT(*) as name_count\n",
        "                                                FROM customer\n",
        "                                                    GROUP BY first_name\n",
        "                                                        HAVING COUNT(*) > 1\n",
        "                                                            ORDER BY name_count DESC, first_name\n",
        "                                                            \"\"\")\n",
        "\n",
        "                                                            # Show the results\n",
        "sql_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpgEWVRtxO_y",
        "outputId": "7005f2cc-4649-4663-cd86-f11875ace6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|first_name|name_count|\n",
            "+----------+----------+\n",
            "|     JAMIE|         2|\n",
            "|    JESSIE|         2|\n",
            "|     KELLY|         2|\n",
            "|    LESLIE|         2|\n",
            "|    MARION|         2|\n",
            "|     TERRY|         2|\n",
            "|     TRACY|         2|\n",
            "|    WILLIE|         2|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qGmjBqDErO"
      },
      "source": [
        "### Question 6.2\n",
        "\n",
        "  2. using the Spark Dataframe API (printing the result once more)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, col\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read customer table from PostgreSQL\n",
        "customer_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"customer\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # DataFrame operations to find duplicate first names\n",
        "df_result = customer_df.groupBy(\"first_name\") \\\n",
        "                                            .agg(count(\"*\").alias(\"name_count\")) \\\n",
        "                                                .filter(col(\"name_count\") > 1) \\\n",
        "                                                    .orderBy(col(\"name_count\").desc(), col(\"first_name\"))\n",
        "\n",
        "                                                    # Show the results\n",
        "df_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdFOErtxYzL",
        "outputId": "8e971e59-7642-49b5-9599-e1464df25002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|first_name|name_count|\n",
            "+----------+----------+\n",
            "|     JAMIE|         2|\n",
            "|    JESSIE|         2|\n",
            "|     KELLY|         2|\n",
            "|    LESLIE|         2|\n",
            "|    MARION|         2|\n",
            "|     TERRY|         2|\n",
            "|     TRACY|         2|\n",
            "|    WILLIE|         2|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA56WFXXDqrm"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL):\n",
        "\n",
        "```\n",
        "SELECT\n",
        "   staff.first_name\n",
        "   ,staff.last_name\n",
        "   ,SUM(payment.amount)\n",
        " FROM payment\n",
        "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
        " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2007-02-01'\n",
        " GROUP BY\n",
        "   staff.last_name\n",
        "   ,staff.first_name\n",
        " ORDER BY SUM(payment.amount)\n",
        " ;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session with PostgreSQL JDBC configuration\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as spark_sum, col\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session with PostgreSQL JDBC driver\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Read payment table\n",
        "payment_df = spark.read \\\n",
        "                .format(\"jdbc\") \\\n",
        "                    .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                        .option(\"dbtable\", \"payment\") \\\n",
        "                            .option(\"user\", \"postgres\") \\\n",
        "                                .option(\"password\", \"test\") \\\n",
        "                                    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                        .load()\n",
        "\n",
        "                                        # Read staff table\n",
        "staff_df = spark.read \\\n",
        "                                            .format(\"jdbc\") \\\n",
        "                                                .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "                                                    .option(\"dbtable\", \"staff\") \\\n",
        "                                                        .option(\"user\", \"postgres\") \\\n",
        "                                                            .option(\"password\", \"test\") \\\n",
        "                                                                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "                                                                    .load()\n",
        "\n",
        "                                                                    # Convert the SQL query to DataFrame operations with correct column references\n",
        "result_df = payment_df \\\n",
        "                                                                        .join(staff_df, payment_df.staff_id == staff_df.staff_id, \"inner\") \\\n",
        "                                                                            .filter((col(\"payment_date\") >= \"2007-01-01\") & (col(\"payment_date\") <= \"2007-02-01\")) \\\n",
        "                                                                                .groupBy(col(\"first_name\"), col(\"last_name\")) \\\n",
        "                                                                                    .agg(spark_sum(\"amount\").alias(\"total_amount\")) \\\n",
        "                                                                                        .orderBy(col(\"total_amount\").desc())\n",
        "\n",
        "                                                                                        # Show results\n",
        "print(\"Staff Payment Summary (Jan 1 - Feb 1, 2007):\")\n",
        "print(\"===========================================\")\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DoXCqmy0fvG",
        "outputId": "82b17fc4-821a-4536-8c85-a8b9fa1dd684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Staff Payment Summary (Jan 1 - Feb 1, 2007):\n",
            "===========================================\n",
            "+----------+---------+------------+\n",
            "|first_name|last_name|total_amount|\n",
            "+----------+---------+------------+\n",
            "+----------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqv7FoidJiBJ"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
        "        .config(\"spark.driver.extraClassPath\", f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "            # Get Spark context\n",
        "sc = spark.sparkContext\n",
        "\n",
        "            # Check if we're on driver by examining local properties\n",
        "is_driver = sc._jsc.sc().isLocal()\n",
        "\n",
        "            # Get deployment mode\n",
        "deploy_mode = spark.conf.get(\"spark.submit.deployMode\", \"client\")\n",
        "\n",
        "            # Get master URL\n",
        "master_url = sc.master\n",
        "\n",
        "print(f\"Is running locally (driver): {is_driver}\")\n",
        "print(f\"Deployment mode: {deploy_mode}\")\n",
        "print(f\"Master URL: {master_url}\")\n",
        "\n",
        "            # Additional configuration details\n",
        "print(\"\\nSpark Configuration Details:\")\n",
        "print(\"============================\")\n",
        "print(f\"Driver Host: {spark.conf.get('spark.driver.host', 'Not Set')}\")\n",
        "print(f\"Driver Port: {spark.conf.get('spark.driver.port', 'Not Set')}\")\n",
        "print(f\"App Name: {sc.appName}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezcUMuEC1clY",
        "outputId": "3f5a4c51-7fa3-47d4-e6ae-3701c54b6391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is running locally (driver): True\n",
            "Deployment mode: client\n",
            "Master URL: local[*]\n",
            "\n",
            "Spark Configuration Details:\n",
            "============================\n",
            "Driver Host: 5f82e67c07fa\n",
            "Driver Port: 33647\n",
            "App Name: pyspark-shell\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}