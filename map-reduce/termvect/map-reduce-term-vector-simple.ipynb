{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTgjnT8sX4em"
      },
      "source": [
        "# Map Reduce\n",
        "\n",
        "This notebook is performing map reduce in a simplified manner in Python. Distribution of compute to different nodes is not done here; the purpose rather is to explore how to implement a map or reduce function, assuming that the functionality is provided akin to the libraries mentioned in [Dean and Ghemawat](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).\n",
        "\n",
        "\n",
        "This notebook comprises a section defining identity mappers and reducers, along with a `run` method which you may change if necessary. An intermediate sort function is also provided.\n",
        "\n",
        "Implement the `mapper` and `reducer` in the Term Vectors section, and use the run cell as provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUkK5N8TX4en",
        "outputId": "d705ca55-f4ad-4b3b-e4b4-4c301501a13c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "%config Completer.use_jedi = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVIOcboaX4eo"
      },
      "outputs": [],
      "source": [
        "# Empty MAPPER\n",
        "def mapper(key, value):\n",
        "    \"\"\"\n",
        "    Our user defined mapper function .\n",
        "    : param key :\n",
        "    : param value :\n",
        "    \"\"\"\n",
        "    yield (key, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBmVhi1KX4eo"
      },
      "outputs": [],
      "source": [
        "# Empty REDUCER\n",
        "\n",
        "def reducer(key , list_value):\n",
        "    \"\"\"\n",
        "    User defined reducer.\n",
        "    : param key :\n",
        "    : param list_value :\n",
        "    \"\"\"\n",
        "    yield (key, list_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seVNlcl5X4eo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def cleaner(line):\n",
        "    # lowercase all words and get alphabetical char only and keeping\n",
        "    # apostrophe for time being\n",
        "    words = re.findall(r'[a-z\\']+' , line.lower())\n",
        "    for word in words :\n",
        "        # we will omit apostrophe ' s assuming users won't type them in a search\n",
        "        word = word.replace(\"'\" , '')\n",
        "        if not (word is '' or word in stopwords.words('english')):\n",
        "            yield word\n",
        "\n",
        "\n",
        "\n",
        "def intermediate_sort(data):\n",
        "    \"\"\"\n",
        "    collect by key\n",
        "    \"\"\"\n",
        "    data = sorted ( data )\n",
        "    return [(k, list(tuple(zip(*g))[1])) for k, g  in groupby(data , itemgetter(0))]\n",
        "\n",
        "\n",
        "\n",
        "def run(sources_dict):\n",
        "    \"\"\"\n",
        "    Since we are focusing on the mapper and reducer functions here we have to\n",
        "    provide the boiler plate code that a MapReduce library typically would . This\n",
        "    function does that in a simple way (we ignore distributing it for now).\n",
        "    : param sources_dict : dictionary where (key,fqfilename), for example ('doc_id','/home/fileX')\n",
        "    \"\"\"\n",
        "    map_result =[]\n",
        "    reduce_result =[]\n",
        "    # open the files and apply map to each of them ( could be done in parallel ,\n",
        "    # but we prefer to keep it simple ) .\n",
        "    for k , v in sources_dict.items():\n",
        "        # do map per source\n",
        "        # this could happen in its own process / worker typically\n",
        "        f = open(v, 'r')\n",
        "        map_result += list(mapper(k, f.read()))\n",
        "        f.close()\n",
        "#         ::alt\n",
        "#          with open(v, 'r') as f:\n",
        "#             for line in f.readlines():\n",
        "#                 map_result += list(mapper(k, line))\n",
        "    # this would be written to disk in the original paradigm ,\n",
        "    # but we keep it in memory for ease of use\n",
        "    intermediate_result = intermediate_sort(map_result)\n",
        "    # now that the data has been ' collected ' and grouped by key it can be handed\n",
        "    # to the reducers . They would run over partitions or chunks usually , but we\n",
        "    # will just iterate through the keys we have and call them\n",
        "    for elem in intermediate_result:\n",
        "        reduce_result.append(list(reducer(elem [0], elem [1])))\n",
        "    return map_result, intermediate_result, reduce_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTs8cvafX4ep",
        "outputId": "7932eb3c-dcb3-44da-d318-a602324f12c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('D1', ['D1 : the cat sat on the mat\\n'])],\n",
              " [('D2', ['D2 : the dog sat on the log\\n'])]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# EXAMPLE\n",
        "!mkdir -p input/\n",
        "!echo -e 'D1 : the cat sat on the mat' > input/d1.txt\n",
        "!echo -e 'D2 : the dog sat on the log' > input/d2.txt\n",
        "\n",
        "_, _, res = run({'D1': 'input/d1.txt' , 'D2': 'input/d2.txt'})\n",
        "\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2UML8RFX4ep"
      },
      "source": [
        "# Term Vector\n",
        "\n",
        "The paper states:\n",
        "\n",
        "> Term-Vector per Host: A term vector summarizes the\n",
        "most important words that occur in a document or a set\n",
        "of documents as a list of 〈word, frequency〉 pairs. The\n",
        "map function emits a 〈hostname, term vector〉\n",
        "pair for each input document (where the hostname is\n",
        "extracted from the URL of the document). The re-\n",
        "duce function is passed all per-document term vectors\n",
        "for a given host. It adds these term vectors together,\n",
        "throwing away infrequent terms, and then emits a final\n",
        "〈hostname, term vector〉 pair.\n",
        "\n",
        "As for\n",
        "\n",
        "> throwing away infrequent terms\n",
        "\n",
        "Write your code in such a way that only terms occurring at least twice are retained.\n",
        "\n",
        "Hint:\n",
        "  * Consider how they use the word 'frequency' elsewhere in the paper.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUXMzTWUX4ep"
      },
      "outputs": [],
      "source": [
        "# your mapper\n",
        "def mapper(key, value):\n",
        "    yield (key, value)\n",
        "\n",
        "def reducer(key , list_value):\n",
        "    yield (key, list_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qX7RozyMX4ep",
        "outputId": "1a57bcf2-194f-4ef4-a872-07c8adde1ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'page1.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4874c6a5529c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'www.somesite.com/page/1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'page1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'www.somesite.com/page/2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'page2.txt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-eefcc1524b63>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(sources_dict)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# do map per source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# this could happen in its own process / worker typically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mmap_result\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'page1.txt'"
          ]
        }
      ],
      "source": [
        "x, y, res = run({'www.somesite.com/page/1': 'page1.txt', 'www.somesite.com/page/2': 'page2.txt'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NPLLvB_aX4ep",
        "outputId": "eda3d0d4-4eb7-44f6-f812-3c7fee53924f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'list_value' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c3a9cb326748>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                                 \u001b[0;31m# Check if list_value is already a list of term vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                                                                \u001b[0;32mfor\u001b[0m \u001b[0mterm_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                    \u001b[0mcombined_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'list_value' is not defined"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def mapper(key, value):\n",
        "    \"\"\"\n",
        "    Our user defined mapper function.\n",
        "    :param key: document URL\n",
        "    :param value: document content\n",
        "    \"\"\"\n",
        "    hostname = key.split('/')[2]  # Extract hostname from URL\n",
        "    term_vector = Counter(cleaner(value))\n",
        "    yield (hostname, dict(term_vector))\n",
        "\n",
        "def reducer(key, list_value):\n",
        "     \"\"\"\n",
        "     User defined reducer.\n",
        "     :param key: hostname\n",
        "     :param list_value: list of term vectors for the host\n",
        "      \"\"\"\n",
        "     combined_vector = Counter()\n",
        "\n",
        "                                                                # Check if list_value is already a list of term vectors\n",
        "if isinstance(list_value, list) and all(isinstance(item, dict) for item in list_value):\n",
        "                                                               for term_vector in list_value:\n",
        "                                                                   combined_vector.update(term_vector)\n",
        "                                                               else:\n",
        "                                                                                                    # If it's a single term vector, just update with it\n",
        "                                                                  combined_vector.update(list_value)\n",
        "from collections import Counter\n",
        "\n",
        "def mapper(key, value):\n",
        "    \"\"\"\n",
        "    Our user defined mapper function.\n",
        "    :param key: document URL\n",
        "    :param value: document content\n",
        "    \"\"\"\n",
        "    hostname = key.split('/')[2]  # Extract hostname from URL\n",
        "    term_vector = Counter(cleaner(value))\n",
        "    yield (hostname, dict(term_vector))\n",
        "\n",
        "def reducer(key, list_value):\n",
        "     \"\"\"\n",
        "     User defined reducer.\n",
        "     :param key: hostname\n",
        "     :param list_value: list of term vectors for the host\n",
        "      \"\"\"\n",
        "     combined_vector = Counter()\n",
        "\n",
        "     # Check if list_value is already a list of term vectors\n",
        "     if isinstance(list_value, list) and all(isinstance(item, dict) for item in list_value):\n",
        "         for term_vector in list_value:\n",
        "             combined_vector.update(term_vector)\n",
        "     else:\n",
        "         # If it's a single term vector, just update with it\n",
        "         combined_vector.update(list_value)\n",
        "\n",
        "     filtered_vector = {word: count for word, count in combined_vector.items() if count >= 2}\n",
        "     yield (key, filtered_vector)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bdt-a3",
      "language": "python",
      "name": "bdt-a3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}